{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.constants as c\n",
    "import astropy.units as u\n",
    "from scipy.interpolate import interp2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "import twopoppy\n",
    "import dsharp_opac as op\n",
    "\n",
    "from dipsy_functions import get_powerlaw_dust_distribution\n",
    "from dipsy_functions import bplanck\n",
    "\n",
    "au      = c.au.cgs.value\n",
    "year    = (1*u.year).cgs.value\n",
    "c_light = c.c.cgs.value\n",
    "jy_sas  = (1 * u.Jy / u.arcsec**2).cgs.value\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load opacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(op.get_datafile('default_opacities.npz')) as f:\n",
    "    a_op      = f['a']\n",
    "    lam_op    = f['lam']\n",
    "    k_abs     = f['k_abs']\n",
    "    k_sca     = f['k_sca']\n",
    "    rho_s_op  = f['rho_s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = twopoppy.args()\n",
    "\n",
    "# make sure we use the same grain density as in the opacities\n",
    "\n",
    "args.rhos = rho_s_op[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.print_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = twopoppy.wrapper.model_wrapper(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some time snapshot index\n",
    "it = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.loglog(res.x / au, res.a_fr[it, :], label='fragmentation')\n",
    "ax.loglog(res.x / au, res.a_dr[it, :], label='drift')\n",
    "ax.loglog(res.x / au, res.a_t[it, :], 'k--', label='$a_\\mathrm{max}$')\n",
    "ax.set_ylim(1e-4, 1e4)\n",
    "ax.set_title(f'time = {res.timesteps[it] / year:.2g} yr')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.loglog(res.x / au, res.sigma_g[it, :], label='gas')\n",
    "ax.loglog(res.x / au, res.sigma_d[it, :], label='dust')\n",
    "ax.set_ylim(1e-4, 1e4)\n",
    "ax.set_title(f'time = {res.timesteps[it] / year:.2g} yr')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot size distribution from the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "cc = ax.pcolormesh(res.x / au, res.a, np.log10(res.sig_sol), vmin=-10, vmax=1)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "plt.colorbar(cc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create power-law size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, a_i, sig_da = get_powerlaw_dust_distribution(res.sigma_d[it, :], res.a_t[it, :], a0=args.a0, na=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "cc = ax.pcolormesh(res.x / au, a_i, np.log10(sig_da.T), vmin=-10, vmax=1)#, edgecolor='k')\n",
    "ax.loglog(res.x / au, res.a_t[it, :], 'r')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(a[[0, -1]])\n",
    "#ax.set_xlim(2e-1, 4e-1)\n",
    "#ax.set_ylim(6e0, 2e1)\n",
    "plt.colorbar(cc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Intensity profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wavelengths\n",
    "\n",
    "lam_obs = [0.087, 0.1, 0.3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lam = len(lam_obs)\n",
    "\n",
    "I_nu = np.zeros([n_lam, len(res.x)])\n",
    "tau  = np.zeros([n_lam, len(res.x)])\n",
    "k_a  = np.zeros([n_lam, len(a)])\n",
    "\n",
    "for ilam, _lam in enumerate(lam_obs):\n",
    "    nu_obs = c_light/_lam\n",
    "\n",
    "    # interpolate on our wavelength and size grid\n",
    "\n",
    "    f_interp = interp2d(np.log10(lam_op), np.log10(a_op), np.log10(k_abs))\n",
    "    k_a[ilam, :] = 10.**f_interp(np.log10(_lam), np.log10(a))[:, 0]\n",
    "\n",
    "    ## Calculate intensity profile\n",
    "\n",
    "    tau[ilam, :]  = (sig_da * k_a[ilam,:].T).sum(-1)\n",
    "    I_nu[ilam, :] = bplanck(nu_obs, res.T) * (1 - np.exp(-tau[ilam, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "for _lam, _Inu in zip(lam_obs, I_nu):\n",
    "    ax.loglog(res.x / au, _Inu/jy_sas, label=f'$\\lambda = {_lam * 10:.2g}$ mm')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(1e-2, 1e2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the dust line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_powerlaw(x, y0, p, xout, noise):\n",
    "    return y0 * (x/x[0])**p * (x < xout) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(params, x, data):\n",
    "    \n",
    "    if params[0]<1e-50:\n",
    "        return -1e300\n",
    "    if np.abs(params[1])>10:\n",
    "        return -1e300\n",
    "    if params[3]<1e-50:\n",
    "        return -1e300\n",
    "    \n",
    "    model = truncated_powerlaw(x, *params)\n",
    "    rmsd = (data - model)**2\n",
    "    # we ignore points that are too far away from the model\n",
    "    #rmsd[rmsd>(100 * noise)**2] = 0.0\n",
    "    rmsd /= 2 * noise**2\n",
    "\n",
    "    return -rmsd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 1e-3 * jy_sas\n",
    "x     = np.linspace(res.x[0], 1e3 * au, 100)\n",
    "data  = np.interp(x, res.x, I_nu[0, :] + noise * np.random.randn(args.nr))\n",
    "p0    = [data[0], -0.5, 100 * au, noise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwalkers = 40\n",
    "nburnin = 200\n",
    "nsteps = 6000\n",
    "sampler = emcee.EnsembleSampler(nwalkers, len(p0), lnprob, args=[x, data])\n",
    "\n",
    "inisamples = np.array([\n",
    "    p0[0] * 10**(-2 + 4 * np.random.rand(nwalkers)),\n",
    "    p0[1] + (-1 + 2 * np.random.rand(nwalkers)),\n",
    "    200 * au * np.random.rand(nwalkers),\n",
    "    noise * 10**(-2 + np.random.rand(nwalkers))]).T\n",
    "\n",
    "# first burn in to keep the ones with reasonable acceptance fraction\n",
    "\n",
    "burnin = sampler.run_mcmc(inisamples, nburnin)\n",
    "good = inisamples[sampler.acceptance_fraction>0.25, :]\n",
    "inisamples = good[np.random.choice(np.arange(len(good)), size=nwalkers)]\n",
    "\n",
    "# second burn in to keep the ones with higher probability\n",
    "\n",
    "sampler.reset()\n",
    "burnin = sampler.run_mcmc(inisamples, nburnin)\n",
    "final_prob = sampler.lnprobability[:, -1]\n",
    "good = np.arange(nwalkers)[final_prob > np.sort(final_prob)[nwalkers//2]]\n",
    "inisamples = inisamples[np.random.choice(good, size=nwalkers)]\n",
    "\n",
    "sampler.reset()\n",
    "output = sampler.run_mcmc(inisamples, nsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(np.arange(nsteps), -sampler.lnprobability.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_time = sampler.get_autocorr_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = int(5 * acc_time.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chain = sampler.chain[:, discard:, :].reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.loglog(x / au, data, label='data')\n",
    "ax.loglog(x / au, truncated_powerlaw(x, *p0), 'k--', label='guess')\n",
    "for sample in flat_chain[-10:,:]:\n",
    "    ax.loglog(x / au, truncated_powerlaw(x, *sample), lw=0.5, alpha=0.5)\n",
    "\n",
    "ax.set_ylim(1e-16, 1e-10)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_data = flat_chain.copy()\n",
    "\n",
    "corner_data[:, 0] = np.log10(corner_data[:, 0])\n",
    "corner_data[:, 2] /= au\n",
    "corner_data[:, 3] = np.log10(corner_data[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(corner_data, range=[[-11, -10], [-3, 3], [1, 200], [-18, -14]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_mcmc = flat_chain.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nelder-Mead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func(params, x, data):\n",
    "    return -lnprob(params, x, data)\n",
    "\n",
    "opt_res = minimize(obj_func, p0, args=(x, data), method='Nelder-Mead', options={'disp':True})\n",
    "p_nm = opt_res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LM Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lm, cov = curve_fit(truncated_powerlaw, x, data, p0=p0, sigma= data * 0.05 + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.loglog(x / au, data, label='data')\n",
    "ax.loglog(res.x / au, truncated_powerlaw(res.x, *p_mcmc), label='MCMC')\n",
    "ax.loglog(res.x / au, truncated_powerlaw(res.x, *p_nm), label='NM')\n",
    "ax.loglog(res.x / au, truncated_powerlaw(res.x, *p_lm), label='LM')\n",
    "ax.set_ylim(1e-16, 1e-10)\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
